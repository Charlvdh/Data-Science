# -*- coding: utf-8 -*-
def get_predictions(q, since):
    """Back_end_integration.ipynb

    Automatically generated by Colaboratory.

    Original file is located at: Removed for security reasons
        
    # Purpose

    The purpose of this notebook is to integrate the backend portions of the Rep Manager Application.

    The back end components include:
    1.  The pulling of tweets from twitter based on a search query provide by the user
    2. The cleaning of these tweets
    3. The preperation of the tweets to be fed into the classification model
    4. The feeding of the tweets into the model and the receiving of the outputs
    5.  The segregation of the outputs into good and bad by rank
    6. The generation of a dataframe to be used by the front end to create animations and provide insight
    """

    """## 1. The pulling of tweets from twitter based on a search query provide by the user

    Steps:

    - Install python-twitter library
    - Import the library
    - Initialise secret tokens for authentication
    - Initialise twitter object using tokens
    - Set up query parameters (In app will ask for user input)
    - Run query and store results in a dataframe for further processing
    """

    # Import API library
    import twitter

    # Initialise secret tokens
    ACCESS_TOKEN = "Removed for security reasons"
    ACCESS_SECRET = "Removed for security reasons"


    CONSUMER_KEY = "Removed for security reasons"
    CONSUMER_SECRET = "Removed for security reasons"

    # Initialise twitter object
    t = twitter.Api(consumer_key=CONSUMER_KEY,
                    consumer_secret=CONSUMER_SECRET,
                    access_token_key=ACCESS_TOKEN,
                    access_token_secret=ACCESS_SECRET)

    # Design raw query

    # See these links on how to structure the query
    # https://developer.twitter.com/en/docs/tweets/search/api-reference/get-search-tweets
    # https://developer.twitter.com/en/docs/tweets/search/guides/standard-operators

    # Determine query arguments
    q = q      #"@DeloitteSA"   # query string examples are "@DeloitteSA", "#Deloitte", "Accounting" "Scandal", "'Accounting Scandal'", etc.
    result_type = "recent"        # can be "mixed", "recent" or "popular"
    since = since   #"2018-05-19"          # date
    count = "100"                 # number of tweets to be returned (max 100)

    def construct_raw_query(q, since, result_type="popular", count=100):
      """
      Takes in query arguments to construct raw query used to query twitter and return tweets

      Parameters:
      - q - String with query terms
      - result_type - can be "mixed", "recent" or "popular"
      - since - date to start searching
      - count -


      Returns:
      - raw_query -- Query used
      """

      # For q, convert " ", @ or # to percent encoded tokens
      if "@" in q:
        q = q.replace("@", "%40")
      if "#" in q:
        q = q.replace("@", "%23")
      if " " in q:
        q = q.replace(" ", "%20")

      raw_query=f"q={q}&result_type={result_type}&since={since}&count={count}"

      return raw_query

    raw_query = construct_raw_query(q, since, result_type=result_type, count=100)

    results = t.GetSearch(raw_query = raw_query)

    # Store results in dataframe for further processing
    import pandas as pd

    # Only need the datetime and text
    df = pd.DataFrame(columns=["created_at","text"])

    # Create dataframe
    for i, status in enumerate(results):
      df.loc[i,"created_at"] = status.created_at
      df.loc[i,"text"] = status.text

    # Convert datetime to datetime from object (str)
    df["created_at"] = pd.to_datetime(df["created_at"])

    df.head()

    """## 2. Tweet cleaning

    Steps:

    - Remove links and @usernames
    - Remove RT's (both the text and any duplicates after the first)
    - Convert to lower case
    - Remove new line characters
    - Remove #text
    - Remove punctuation
    - Remove multiple spaces
    - Strip leading and trailing spaces
    - Add original text back as seperate column
    - Drop duplicates
    - Reindex the df
    """

    # Define regex patterns
    link_pat = r"https?[a-zA-Z0-9@:%._\+~#=\/]{2,256} ?"
    user_pat = r"@[a-zA-Z0-9_]{1,20} ?"
    hashtag_pat = r"#[a-zA-Z0-9_]{1,20} ?"

    df1 = df.copy()

    # Remove links and usernames
    df1["text"] = df["text"].str.replace(link_pat, "").str.replace(user_pat, "")

    # Remove RT :
    df1["text"] = df1["text"].str.replace("RT :", "")

    # Convert to lowercase
    df1["text"] = df1["text"].str.lower()

    # Remove newline characters
    df1["text"] = df1["text"].str.replace("\n", "")

    # Remove #text
    df1["text"] = df1["text"].str.replace(hashtag_pat, "")

    # Remove punctuation
    import string
    df1["text"] = df1["text"].str.replace('[{}]'.format(string.punctuation), '')

    # Remove multiple spaces
    df["text"] = df["text"].str.replace(r" {2,10}", ' ')

    # Strip leading and trailing spaces
    df1["text"] = df1["text"].str.strip()

    # Add original text back as seperate column
    df1["original_text"] = df["text"]

    # Drop duplicates
    df1.drop_duplicates(subset=["text"], inplace=True)

    # Reindex the df
    df1 = df1.reset_index()

    """## 3. The preparation of the tweets to be fed into the classification model

    Steps:

    - Load word_to_int
    - Convert tweet into vector
    - Pad tweet
    """

    import torch

    # Define function to load word_to_int from file
    def load_word_to_int(filepath):

        checkpoint = torch.load(filepath)

        word_to_int = checkpoint["word_to_int"]
        int_to_word = checkpoint["int_to_word"]

        print("--Checkpoint loaded--")

        return word_to_int, int_to_word

    # Obtain word_to_int
    word_to_int, int_to_word = load_word_to_int("w2i_safe.pth")

    # Convert words to ints
    def tokenize_tweet(tweet):
        tweet = tweet.lower() # lowercase

        # splitting by spaces
        tweet_words = tweet.split()

        # tokens
        tweet_ints = []
        tweet_ints.append([word_to_int[word] for word in tweet_words if word in word_to_int.keys()])

        return tweet_ints[0]

    # Pad the features
    import numpy as np

    def pad_features(tweet_ints, seq_length=30):

      # Create list of 0s of length seq_length
      features = [0 for i in range(seq_length)]

      # Fill in at applicable indexes
      for i in range(len(tweet_ints)):
        features[i] = tweet_ints[i]

      return np.array([features])

    from torch import nn

    # Define model
    class SentimentRNN(nn.Module):
        """
        The RNN model that will be used to perform Sentiment analysis.
        """

        def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, drop_prob=0.5):
            """
            Initialize the model by setting up the layers.
            """
            super(SentimentRNN, self).__init__()

            self.output_size = output_size
            self.n_layers = n_layers
            self.hidden_dim = hidden_dim

            # embedding and LSTM layers
            self.embedding = nn.Embedding(vocab_size, embedding_dim)
            self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers,
                                dropout=drop_prob, batch_first=True)

            # dropout layer
            self.dropout = nn.Dropout(0.3)

            # linear and sigmoid layers
            self.fc = nn.Linear(hidden_dim, output_size)
            self.sig = nn.Sigmoid()


        def forward(self, x, hidden):
            """
            Perform a forward pass of the model on some input and hidden state.
            """
            batch_size = x.size(0)

            # embeddings and lstm_out
            x = x.long()
            embeds = self.embedding(x)
            lstm_out, hidden = self.lstm(embeds, hidden)

            # stack up lstm outputs
            lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)

            # dropout and fully-connected layer
            out = self.dropout(lstm_out)
            out = self.fc(out)
            # sigmoid function
            sig_out = self.sig(out)

            # reshape to be batch_size first
            sig_out = sig_out.view(batch_size, -1)
            sig_out = sig_out[:, -1] # get last batch of labels

            # return last sigmoid output and hidden state
            return sig_out, hidden


        def init_hidden(self, batch_size):
            ''' Initializes hidden state '''
            # Create two new tensors with sizes n_layers x batch_size x hidden_dim,
            # initialized to zero, for hidden state and cell state of LSTM
            weight = next(self.parameters()).data

            if (torch.cuda.is_available()):
                hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda(),
                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda())
            else:
                hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_(),
                          weight.new(self.n_layers, batch_size, self.hidden_dim).zero_())

            return hidden

    # Define function to load net from file
    def load_checkpoint(filepath):

        checkpoint = torch.load(filepath, map_location="cpu")

        vocab_size = checkpoint['vocab_size']
        output_size = checkpoint['output_size']
        embedding_dim = checkpoint['embedding_dim']
        hidden_dim = checkpoint['hidden_dim']
        n_layers = checkpoint['n_layers']

        model = SentimentRNN(vocab_size, output_size, embedding_dim, hidden_dim, n_layers)
        model.load_state_dict(checkpoint['state_dict'])

        print("--Checkpoint loaded--")

        return model

    # Initialise net
    net = load_checkpoint("checkpoint_safe_updated.pth")

    # Define prediction function
    def predict(tweet, sequence_length=30):

        net.eval()

        # tokenize tweet
        tweet_ints = tokenize_tweet(tweet)

        # pad tokenized sequence
        seq_length = sequence_length
        features = pad_features(tweet_ints, seq_length)

        # convert to tensor to pass into the model
        feature_tensor = torch.from_numpy(features)

        batch_size = feature_tensor.size(0)

        # initialize hidden state
        h = net.init_hidden(batch_size)

        if(torch.cuda.is_available()):
            feature_tensor = feature_tensor.cuda()

        # get the output from the model
        output, h = net(feature_tensor, h)

        # convert output probabilities to predicted class (0 or 1)
        hard_pred = torch.round(output.squeeze()).item()
        soft_pred = output.item()

        return hard_pred, soft_pred, tweet

    # Create temp df to hold predictions
    temp_pred_df = df1["text"].apply(predict)
    # Convert to dataframe with proper headers
    pred_df = pd.DataFrame(temp_pred_df.tolist(), columns=["hard_pred","soft_pred","tweet"])
    # Append the original text and date for use in visialisations on the front end
    full_pred_df = pd.concat([pred_df, df1[['original_text','created_at']]], axis=1)



    """# Topic modeling using Latent Dirichlet Allocation
    """

    def get_topics(pred_df):

      # Setup
      import nltk
      nltk.download('wordnet')
      from nltk.corpus import wordnet as wn
      from nltk.stem.wordnet import WordNetLemmatizer
      from gensim import corpora
      import gensim

      # Get stop words
      nltk.download('stopwords')
      en_stop = set(nltk.corpus.stopwords.words('english'))

      # Set num_topics and num_words
      num_topics=3
      num_words=3

      # Build different topic for each of the sentiments
      for sentiment in (0,1):

        # Define function to split tweets into word tokens
        def tokenize(text):
            return text.split()

        # We use NLTKâ€™s Wordnet to find the meanings of words, synonyms, antonyms, and more.
        # Use WordNetLemmatizer to get the root word.
        def get_lemma(word):
            lemma = wn.morphy(word)
            if lemma is None:
                return word
            else:
                return lemma

        def get_lemma2(word):
            return WordNetLemmatizer().lemmatize(word)

        # Define a function to prepare the text for topic modelling:
        def prepare_text_for_lda(text):
            tokens = tokenize(text)
            tokens = [token for token in tokens if len(token) > 4]
            tokens = [token for token in tokens if token not in en_stop]
            tokens = [get_lemma(token) for token in tokens]
            return tokens

        tweets = pred_df.loc[pred_df["hard_pred"] == sentiment, "tweet"]

        # Append tweets to list
        tweet_data = []
        for tweet in tweets:
            tokens = prepare_text_for_lda(tweet)
            tweet_data.append(tokens)

        # Create a dictionary from the data, then convert to bag-of-words corpus
        dictionary = corpora.Dictionary(tweet_data)
        corpus = [dictionary.doc2bow(text) for text in tweet_data]

        # Find num_topics topics with num_words words
        ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics = num_topics, id2word=dictionary, passes=15)
        topics = ldamodel.show_topics(num_words=num_words, formatted=False)

        topic_dict = dict(topics)

        # Determine the most relavant topic for each tweet and append to full_pred_df
        def classify_tweet(tweet):
          tweet = prepare_text_for_lda(tweet)
          tweet_bow = dictionary.doc2bow(tweet)
          tweet_dict = dict(ldamodel.get_document_topics(tweet_bow))
          tweet_topic_id = max(tweet_dict, key=tweet_dict.get)
          return tweet_topic_id

        # Create function to get topic from topic_id (upercase words)
        def get_topic_from_id(id):
          return " ".join([word.upper() for (word, prob) in topic_dict[id]])


        pred_df.loc[pred_df["hard_pred"] == sentiment, "topic"] = pred_df["tweet"].apply(classify_tweet).apply(get_topic_from_id)

      return pred_df

    return get_topics(full_pred_df)
