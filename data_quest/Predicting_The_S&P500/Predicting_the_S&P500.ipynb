{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required mudules\n",
    "\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from itertools import combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the csv file as a DataFrame\n",
    "\n",
    "sap = pd.read_csv(\"sphist.csv\", index_col=False)\n",
    "\n",
    "# Convert the Date column to datetime\n",
    "\n",
    "sap[\"Date\"] = pd.to_datetime(sap[\"Date\"])\n",
    "\n",
    "# The dataframe is currently sored by date in descending order\n",
    "# Let's change it to ascending order\n",
    "# Also reset index so starts at 0\n",
    "\n",
    "sap = sap.sort_values(by=\"Date\", ascending=True)\n",
    "\n",
    "sap = sap.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate 5 days moving average, 30 day moving average\n",
    "# 5 day standard deviation and 30 day standerd deviation and and them as columns\n",
    "\n",
    "# ma_5\n",
    "def add_moving_average_col(period):\n",
    "    control = []\n",
    "    col = []\n",
    "    for row in sap.iterrows():\n",
    "        row = row[1]\n",
    "        if len(control) < period:\n",
    "            control.append(row[\"Close\"])\n",
    "            col.append(0)\n",
    "        else:\n",
    "            col.append(np.mean(control))\n",
    "            del control[0]\n",
    "            control.append(row[\"Close\"])\n",
    "    sap[f\"{period}_day_ma\"] = col\n",
    "\n",
    "add_moving_average_col(5)\n",
    "\n",
    "# ma_30\n",
    "add_moving_average_col(30)\n",
    "\n",
    "# std_5\n",
    "def add_moving_std_col(period):\n",
    "    control = []\n",
    "    col = []\n",
    "    for row in sap.iterrows():\n",
    "        row = row[1]\n",
    "        if len(control) < period:\n",
    "            control.append(row[\"Close\"])\n",
    "            col.append(0)\n",
    "        else:\n",
    "            col.append(np.std(control))\n",
    "            del control[0]\n",
    "            control.append(row[\"Close\"])\n",
    "    sap[f\"{period}_day_std\"] = col\n",
    "\n",
    "add_moving_std_col(5)\n",
    "\n",
    "# std_30\n",
    "add_moving_std_col(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also add the prior day close and volume as columns\n",
    "\n",
    "# pr_day_close\n",
    "def prior_day(attribute):\n",
    "    col = sap[attribute].shift(1)\n",
    "    sap[f\"pr_day_{attribute}\".lower()] = col\n",
    "\n",
    "prior_day(\"Close\")\n",
    "\n",
    "# pr-day_volume\n",
    "prior_day(\"Volume\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the first 30 rows as these rows do not have values for\n",
    "# all indicators\n",
    "sap.drop(range(0,31), inplace=True)\n",
    "\n",
    "# Drop na rows\n",
    "sap.dropna(axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split df into train and test sets with train being all datapoints\n",
    "# before 2013-01-01\n",
    "train = sap.loc[sap[\"Date\"] < datetime(year=2013, month=1, day=1)]\n",
    "test = sap.loc[sap[\"Date\"] >= datetime(year=2013, month=1, day=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's build, train and test a model with absolute mean error\n",
    "# as the error metric\n",
    "\n",
    "# For features we can only use metrics we would know at the end\n",
    "# of the previous day\n",
    "all_features = ['5_day_ma', '30_day_ma', '5_day_std', '30_day_std',\n",
    "'pr_day_close', 'pr_day_volume']\n",
    "\n",
    "target = \"Close\"\n",
    "\n",
    "def train_and_test():\n",
    "\n",
    "# In order to find the best combination of columns to use\n",
    "# generate a model for every combination and calculate and\n",
    "# compare error metrics\n",
    "\n",
    "    maes = {}\n",
    "    combinations_list = []\n",
    "\n",
    "    for i in range(1, len(all_features)):\n",
    "\n",
    "        new_combinations = combinations(all_features, i)\n",
    "\n",
    "        for combination in new_combinations:\n",
    "\n",
    "            combinations_as_a_list = list(combination)\n",
    "            # In order to store the list as a dictionary value it is\n",
    "            # necessary to join the items into a string and then\n",
    "            # split them out to a list later\n",
    "            key = '-'.join(combinations_as_a_list)\n",
    "            combinations_list.append(key)\n",
    "\n",
    "    for features in combinations_list:\n",
    "\n",
    "        features = features.split('-')\n",
    "\n",
    "        model = LinearRegression()\n",
    "        model.fit(train[features], train[target])\n",
    "        predictions = model.predict(test[features])\n",
    "\n",
    "        mae = mean_absolute_error(test[target], predictions)\n",
    "\n",
    "        features = '-'.join(features)\n",
    "\n",
    "        maes[features] = mae\n",
    "\n",
    "    # Turn the dict into a dataframe\n",
    "\n",
    "    df = pd.DataFrame.from_dict(maes, orient=\"index\")\n",
    "    df.columns = [\"mae\"]\n",
    "    \n",
    "    # return the dataframe\n",
    "    return df\n",
    "\n",
    "df = train_and_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                          mae\n",
      "30_day_ma-30_day_std-pr_day_close                   10.995101\n",
      "30_day_ma-pr_day_close                              10.995633\n",
      "30_day_ma-pr_day_close-pr_day_volume                10.997554\n",
      "30_day_ma-5_day_std-30_day_std-pr_day_close         10.997932\n",
      "30_day_ma-5_day_std-pr_day_close                    10.998866\n",
      "30_day_ma-30_day_std-pr_day_close-pr_day_volume     10.999435\n",
      "30_day_ma-5_day_std-30_day_std-pr_day_close-pr_...  11.003879\n",
      "30_day_ma-5_day_std-pr_day_close-pr_day_volume      11.004355\n",
      "pr_day_close                                        11.011899\n",
      "pr_day_close-pr_day_volume                          11.012055\n",
      "                                            mae\n",
      "30_day_ma-30_day_std                  31.332082\n",
      "30_day_ma-pr_day_volume               31.692958\n",
      "30_day_ma                             31.964290\n",
      "30_day_std-pr_day_volume             732.865697\n",
      "5_day_std-30_day_std-pr_day_volume   743.740686\n",
      "pr_day_volume                        749.040001\n",
      "5_day_std-pr_day_volume              778.781203\n",
      "5_day_std-30_day_std                 885.544905\n",
      "30_day_std                           899.514230\n",
      "5_day_std                           1006.985388\n"
     ]
    }
   ],
   "source": [
    "# sort the df by mae\n",
    "df_sorted = df.sort_values(by=\"mae\")\n",
    "\n",
    "# Inspect the highest and lowest values\n",
    "print(df_sorted.head(10))\n",
    "print(df_sorted.tail(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pr_day_volume    4\n",
       "5_day_std        4\n",
       "30_day_std       4\n",
       "dtype: int64"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find the most common attributes in the poorly performing models\n",
    "\n",
    "outlier_models = df_sorted.loc[df_sorted[\"mae\"] > 100].index\n",
    "\n",
    "def split_into_attributes(index_values):\n",
    "\n",
    "    attribute_list = []\n",
    "\n",
    "    for model in index_values:\n",
    "        for attribute in model.split(\"-\"):\n",
    "            attribute_list.append(attribute)\n",
    "    return pd.Series(attribute_list).value_counts()\n",
    "\n",
    "split_into_attributes(outlier_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pr_day_close     10\n",
       "30_day_ma         8\n",
       "pr_day_volume     5\n",
       "5_day_std         4\n",
       "30_day_std        4\n",
       "dtype: int64"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find the top attributes in the top 10 models\n",
    "\n",
    "split_into_attributes(df_sorted[:10].index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus we can see that of the 12 attributes in the worst performing models:\n",
    "- pr_day_volume, 5_day_std and 30_day_std all appear 4 times\n",
    "- the models have on average 1.7 attributes\n",
    "- the worst three models and the only ones with error values over 800 are 5_day_std, 30_day_std and 5_day_std-30_day_std\n",
    "\n",
    "Thus it appears that 5_day_std, 30_day_std and 5_day_std-30_day_std are not suitable metrics for predicting the Close price of the S&P 500. This is most probably due to the non-linear nature of the metrics\n",
    "\n",
    "For the 10 best performing models:\n",
    "- pr_day_close appears in every one\n",
    "- pr_day_close is the 9th best performing model\n",
    "- 30_day_ma apears in the top 8 best performing models\n",
    "- the best performing model is 30_day_ma-30_day_std-pr_day_close which seems strange and is most likely a coincidence due to the otherwise bad performance of the 30_day_std metric\n",
    "- the second best performing model is 0_day_ma-pr_day_close \n",
    "\n",
    "Thus it appears that pr_day_close and 30_day_ma are the most suitable metrics and that the actual best performing, reliable model has a mae of 10.995633 rather than 10.995101"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
